// Auto-generated by scripts/generate_chart_data.py â€” do not edit
window.BENCHMARK_DATA = {
  "generated": "sample",
  "models": [
    "qwen3-4b",
    "qwen3-1.7b",
    "llama-3.2-3b",
    "llama-3.2-1b",
    "phi-4-mini",
    "gemma-3-1b",
    "gemma-3-4b",
    "deepseek-r1-1.5b",
    "smollm2-1.7b",
    "ministral-3b",
    "qwen2.5-coder-3b",
    "phi-4-mini-reasoning",
    "deepseek-r1-7b",
    "tinyllama-1.1b"
  ],
  "model_meta": {
    "qwen3-4b": {
      "id": "qwen3-4b",
      "name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0
    },
    "qwen3-1.7b": {
      "id": "qwen3-1.7b",
      "name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7
    },
    "llama-3.2-3b": {
      "id": "llama-3.2-3b",
      "name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2
    },
    "llama-3.2-1b": {
      "id": "llama-3.2-1b",
      "name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0
    },
    "phi-4-mini": {
      "id": "phi-4-mini",
      "name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8
    },
    "gemma-3-1b": {
      "id": "gemma-3-1b",
      "name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0
    },
    "gemma-3-4b": {
      "id": "gemma-3-4b",
      "name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0
    },
    "deepseek-r1-1.5b": {
      "id": "deepseek-r1-1.5b",
      "name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5
    },
    "smollm2-1.7b": {
      "id": "smollm2-1.7b",
      "name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7
    },
    "ministral-3b": {
      "id": "ministral-3b",
      "name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0
    },
    "qwen2.5-coder-3b": {
      "id": "qwen2.5-coder-3b",
      "name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0
    },
    "phi-4-mini-reasoning": {
      "id": "phi-4-mini-reasoning",
      "name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8
    },
    "deepseek-r1-7b": {
      "id": "deepseek-r1-7b",
      "name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0
    },
    "tinyllama-1.1b": {
      "id": "tinyllama-1.1b",
      "name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1
    }
  },
  "quant_levels": [
    "BF16",
    "Q8_0",
    "Q6_K",
    "Q5_K_M",
    "Q4_K_M",
    "Q4_0",
    "Q3_K_M",
    "Q2_K"
  ],
  "quant_meta": {
    "BF16": {
      "id": "BF16",
      "bpw": 16.0
    },
    "Q8_0": {
      "id": "Q8_0",
      "bpw": 8.0
    },
    "Q6_K": {
      "id": "Q6_K",
      "bpw": 6.6
    },
    "Q5_K_M": {
      "id": "Q5_K_M",
      "bpw": 5.7
    },
    "Q4_K_M": {
      "id": "Q4_K_M",
      "bpw": 4.8
    },
    "Q4_0": {
      "id": "Q4_0",
      "bpw": 4.0
    },
    "Q3_K_M": {
      "id": "Q3_K_M",
      "bpw": 3.4
    },
    "Q2_K": {
      "id": "Q2_K",
      "bpw": 2.6
    }
  },
  "tasks": [
    "mmlu",
    "hellaswag",
    "gsm8k",
    "truthfulqa",
    "arc_challenge"
  ],
  "variants": [
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 7.823,
      "scores": {
        "mmlu": 65.1,
        "hellaswag": 76.0,
        "gsm8k": 58.2,
        "truthfulqa": 52.8,
        "arc_challenge": 55.7
      },
      "composite_score": 61.6,
      "tg_ts": 4.9,
      "pp_ts": 27.5,
      "ttft_ms": 18618.2
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 3.912,
      "scores": {
        "mmlu": 63.9,
        "hellaswag": 75.6,
        "gsm8k": 57.7,
        "truthfulqa": 52.2,
        "arc_challenge": 56.0
      },
      "composite_score": 61.1,
      "tg_ts": 8.1,
      "pp_ts": 56.4,
      "ttft_ms": 9078.0
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 3.227,
      "scores": {
        "mmlu": 64.1,
        "hellaswag": 75.1,
        "gsm8k": 57.6,
        "truthfulqa": 52.6,
        "arc_challenge": 55.3
      },
      "composite_score": 60.9,
      "tg_ts": 11.6,
      "pp_ts": 75.6,
      "ttft_ms": 6772.5
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.787,
      "scores": {
        "mmlu": 64.3,
        "hellaswag": 75.3,
        "gsm8k": 57.6,
        "truthfulqa": 52.4,
        "arc_challenge": 55.1
      },
      "composite_score": 60.9,
      "tg_ts": 11.6,
      "pp_ts": 77.4,
      "ttft_ms": 6615.0
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 2.347,
      "scores": {
        "mmlu": 64.6,
        "hellaswag": 74.9,
        "gsm8k": 57.5,
        "truthfulqa": 51.1,
        "arc_challenge": 53.8
      },
      "composite_score": 60.4,
      "tg_ts": 14.6,
      "pp_ts": 101.0,
      "ttft_ms": 5069.3
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.956,
      "scores": {
        "mmlu": 63.8,
        "hellaswag": 75.7,
        "gsm8k": 56.5,
        "truthfulqa": 52.2,
        "arc_challenge": 54.6
      },
      "composite_score": 60.6,
      "tg_ts": 18.7,
      "pp_ts": 109.1,
      "ttft_ms": 4692.9
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.662,
      "scores": {
        "mmlu": 64.1,
        "hellaswag": 75.7,
        "gsm8k": 57.4,
        "truthfulqa": 51.9,
        "arc_challenge": 55.3
      },
      "composite_score": 60.9,
      "tg_ts": 21.8,
      "pp_ts": 137.4,
      "ttft_ms": 3726.3
    },
    {
      "model_id": "qwen3-4b",
      "model_name": "Qwen3-4B",
      "family": "Qwen",
      "params_b": 4.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 1.271,
      "scores": {
        "mmlu": 64.2,
        "hellaswag": 75.0,
        "gsm8k": 57.2,
        "truthfulqa": 51.6,
        "arc_challenge": 56.1
      },
      "composite_score": 60.8,
      "tg_ts": 28.5,
      "pp_ts": 188.6,
      "ttft_ms": 2714.7
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 3.325,
      "scores": {
        "mmlu": 48.9,
        "hellaswag": 63.0,
        "gsm8k": 30.4,
        "truthfulqa": 44.6,
        "arc_challenge": 43.4
      },
      "composite_score": 46.1,
      "tg_ts": 10.7,
      "pp_ts": 73.2,
      "ttft_ms": 6994.5
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 1.662,
      "scores": {
        "mmlu": 48.0,
        "hellaswag": 62.4,
        "gsm8k": 29.1,
        "truthfulqa": 43.9,
        "arc_challenge": 42.5
      },
      "composite_score": 45.2,
      "tg_ts": 20.4,
      "pp_ts": 134.6,
      "ttft_ms": 3803.9
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 1.371,
      "scores": {
        "mmlu": 48.2,
        "hellaswag": 63.0,
        "gsm8k": 30.0,
        "truthfulqa": 43.5,
        "arc_challenge": 42.0
      },
      "composite_score": 45.3,
      "tg_ts": 27.8,
      "pp_ts": 169.7,
      "ttft_ms": 3017.1
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 1.184,
      "scores": {
        "mmlu": 47.4,
        "hellaswag": 61.8,
        "gsm8k": 30.0,
        "truthfulqa": 43.4,
        "arc_challenge": 42.1
      },
      "composite_score": 44.9,
      "tg_ts": 30.2,
      "pp_ts": 213.9,
      "ttft_ms": 2393.6
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 0.997,
      "scores": {
        "mmlu": 47.5,
        "hellaswag": 61.9,
        "gsm8k": 30.2,
        "truthfulqa": 43.7,
        "arc_challenge": 42.3
      },
      "composite_score": 45.1,
      "tg_ts": 32.5,
      "pp_ts": 241.3,
      "ttft_ms": 2121.8
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 0.831,
      "scores": {
        "mmlu": 47.8,
        "hellaswag": 61.8,
        "gsm8k": 29.0,
        "truthfulqa": 44.1,
        "arc_challenge": 42.3
      },
      "composite_score": 45.0,
      "tg_ts": 46.6,
      "pp_ts": 284.0,
      "ttft_ms": 1802.8
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 0.707,
      "scores": {
        "mmlu": 48.6,
        "hellaswag": 61.5,
        "gsm8k": 30.3,
        "truthfulqa": 43.7,
        "arc_challenge": 41.8
      },
      "composite_score": 45.2,
      "tg_ts": 47.5,
      "pp_ts": 341.5,
      "ttft_ms": 1499.3
    },
    {
      "model_id": "qwen3-1.7b",
      "model_name": "Qwen3-1.7B",
      "family": "Qwen",
      "params_b": 1.7,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.54,
      "scores": {
        "mmlu": 47.0,
        "hellaswag": 62.0,
        "gsm8k": 29.7,
        "truthfulqa": 42.4,
        "arc_challenge": 42.4
      },
      "composite_score": 44.7,
      "tg_ts": 70.1,
      "pp_ts": 413.8,
      "ttft_ms": 1237.3
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 6.258,
      "scores": {
        "mmlu": 58.4,
        "hellaswag": 73.5,
        "gsm8k": 45.5,
        "truthfulqa": 48.6,
        "arc_challenge": 51.1
      },
      "composite_score": 55.4,
      "tg_ts": 5.7,
      "pp_ts": 34.9,
      "ttft_ms": 14670.5
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 3.129,
      "scores": {
        "mmlu": 58.8,
        "hellaswag": 73.1,
        "gsm8k": 43.5,
        "truthfulqa": 48.9,
        "arc_challenge": 50.3
      },
      "composite_score": 54.9,
      "tg_ts": 10.1,
      "pp_ts": 72.4,
      "ttft_ms": 7071.8
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 2.582,
      "scores": {
        "mmlu": 59.2,
        "hellaswag": 73.1,
        "gsm8k": 44.7,
        "truthfulqa": 48.1,
        "arc_challenge": 50.8
      },
      "composite_score": 55.2,
      "tg_ts": 14.7,
      "pp_ts": 99.0,
      "ttft_ms": 5171.7
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.23,
      "scores": {
        "mmlu": 58.1,
        "hellaswag": 73.4,
        "gsm8k": 44.2,
        "truthfulqa": 49.4,
        "arc_challenge": 51.1
      },
      "composite_score": 55.2,
      "tg_ts": 16.6,
      "pp_ts": 97.4,
      "ttft_ms": 5256.7
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 1.878,
      "scores": {
        "mmlu": 57.2,
        "hellaswag": 72.9,
        "gsm8k": 43.7,
        "truthfulqa": 49.6,
        "arc_challenge": 50.4
      },
      "composite_score": 54.8,
      "tg_ts": 18.9,
      "pp_ts": 130.7,
      "ttft_ms": 3917.4
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.565,
      "scores": {
        "mmlu": 57.9,
        "hellaswag": 72.9,
        "gsm8k": 44.1,
        "truthfulqa": 49.3,
        "arc_challenge": 50.6
      },
      "composite_score": 55.0,
      "tg_ts": 22.2,
      "pp_ts": 150.5,
      "ttft_ms": 3402.0
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.33,
      "scores": {
        "mmlu": 57.9,
        "hellaswag": 73.0,
        "gsm8k": 43.3,
        "truthfulqa": 49.1,
        "arc_challenge": 50.6
      },
      "composite_score": 54.8,
      "tg_ts": 24.2,
      "pp_ts": 181.1,
      "ttft_ms": 2827.2
    },
    {
      "model_id": "llama-3.2-3b",
      "model_name": "Llama-3.2-3B",
      "family": "Llama",
      "params_b": 3.2,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 1.017,
      "scores": {
        "mmlu": 57.9,
        "hellaswag": 72.8,
        "gsm8k": 44.6,
        "truthfulqa": 48.5,
        "arc_challenge": 50.3
      },
      "composite_score": 54.8,
      "tg_ts": 32.8,
      "pp_ts": 238.6,
      "ttft_ms": 2145.9
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 1.956,
      "scores": {
        "mmlu": 38.3,
        "hellaswag": 56.1,
        "gsm8k": 18.9,
        "truthfulqa": 40.2,
        "arc_challenge": 34.2
      },
      "composite_score": 37.5,
      "tg_ts": 19.1,
      "pp_ts": 112.6,
      "ttft_ms": 4547.1
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 0.978,
      "scores": {
        "mmlu": 38.0,
        "hellaswag": 55.9,
        "gsm8k": 18.4,
        "truthfulqa": 39.7,
        "arc_challenge": 35.1
      },
      "composite_score": 37.4,
      "tg_ts": 37.6,
      "pp_ts": 248.3,
      "ttft_ms": 2062.0
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 0.807,
      "scores": {
        "mmlu": 38.0,
        "hellaswag": 55.4,
        "gsm8k": 17.7,
        "truthfulqa": 40.4,
        "arc_challenge": 35.0
      },
      "composite_score": 37.3,
      "tg_ts": 40.9,
      "pp_ts": 287.9,
      "ttft_ms": 1778.4
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 0.697,
      "scores": {
        "mmlu": 37.4,
        "hellaswag": 55.0,
        "gsm8k": 18.2,
        "truthfulqa": 40.1,
        "arc_challenge": 35.9
      },
      "composite_score": 37.3,
      "tg_ts": 50.0,
      "pp_ts": 361.2,
      "ttft_ms": 1417.5
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 0.587,
      "scores": {
        "mmlu": 37.5,
        "hellaswag": 55.2,
        "gsm8k": 18.9,
        "truthfulqa": 40.1,
        "arc_challenge": 35.8
      },
      "composite_score": 37.5,
      "tg_ts": 64.2,
      "pp_ts": 373.3,
      "ttft_ms": 1371.6
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 0.489,
      "scores": {
        "mmlu": 37.3,
        "hellaswag": 54.9,
        "gsm8k": 18.0,
        "truthfulqa": 39.9,
        "arc_challenge": 34.7
      },
      "composite_score": 37.0,
      "tg_ts": 70.3,
      "pp_ts": 526.6,
      "ttft_ms": 972.3
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 0.416,
      "scores": {
        "mmlu": 37.4,
        "hellaswag": 56.1,
        "gsm8k": 17.4,
        "truthfulqa": 40.1,
        "arc_challenge": 36.2
      },
      "composite_score": 37.4,
      "tg_ts": 85.7,
      "pp_ts": 589.4,
      "ttft_ms": 868.7
    },
    {
      "model_id": "llama-3.2-1b",
      "model_name": "Llama-3.2-1B",
      "family": "Llama",
      "params_b": 1.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.318,
      "scores": {
        "mmlu": 37.0,
        "hellaswag": 55.4,
        "gsm8k": 18.2,
        "truthfulqa": 40.6,
        "arc_challenge": 34.4
      },
      "composite_score": 37.1,
      "tg_ts": 111.7,
      "pp_ts": 775.1,
      "ttft_ms": 660.6
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 7.432,
      "scores": {
        "mmlu": 64.4,
        "hellaswag": 75.1,
        "gsm8k": 61.1,
        "truthfulqa": 50.7,
        "arc_challenge": 56.9
      },
      "composite_score": 61.6,
      "tg_ts": 4.3,
      "pp_ts": 29.6,
      "ttft_ms": 17297.3
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 3.716,
      "scores": {
        "mmlu": 64.3,
        "hellaswag": 73.9,
        "gsm8k": 61.0,
        "truthfulqa": 50.4,
        "arc_challenge": 56.0
      },
      "composite_score": 61.1,
      "tg_ts": 10.2,
      "pp_ts": 60.0,
      "ttft_ms": 8533.3
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 3.066,
      "scores": {
        "mmlu": 62.5,
        "hellaswag": 74.0,
        "gsm8k": 60.7,
        "truthfulqa": 50.6,
        "arc_challenge": 54.5
      },
      "composite_score": 60.5,
      "tg_ts": 10.8,
      "pp_ts": 79.9,
      "ttft_ms": 6408.0
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.648,
      "scores": {
        "mmlu": 62.8,
        "hellaswag": 74.3,
        "gsm8k": 61.6,
        "truthfulqa": 50.0,
        "arc_challenge": 55.2
      },
      "composite_score": 60.8,
      "tg_ts": 12.8,
      "pp_ts": 93.1,
      "ttft_ms": 5499.5
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 2.23,
      "scores": {
        "mmlu": 63.3,
        "hellaswag": 74.5,
        "gsm8k": 62.6,
        "truthfulqa": 50.2,
        "arc_challenge": 55.8
      },
      "composite_score": 61.3,
      "tg_ts": 15.0,
      "pp_ts": 114.4,
      "ttft_ms": 4475.5
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.858,
      "scores": {
        "mmlu": 62.9,
        "hellaswag": 74.9,
        "gsm8k": 60.2,
        "truthfulqa": 49.9,
        "arc_challenge": 55.7
      },
      "composite_score": 60.7,
      "tg_ts": 20.2,
      "pp_ts": 131.5,
      "ttft_ms": 3893.5
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.579,
      "scores": {
        "mmlu": 61.5,
        "hellaswag": 73.1,
        "gsm8k": 60.7,
        "truthfulqa": 50.0,
        "arc_challenge": 55.6
      },
      "composite_score": 60.2,
      "tg_ts": 23.0,
      "pp_ts": 161.7,
      "ttft_ms": 3166.4
    },
    {
      "model_id": "phi-4-mini",
      "model_name": "Phi-4-Mini",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 1.208,
      "scores": {
        "mmlu": 62.2,
        "hellaswag": 74.2,
        "gsm8k": 60.8,
        "truthfulqa": 50.5,
        "arc_challenge": 55.7
      },
      "composite_score": 60.7,
      "tg_ts": 27.8,
      "pp_ts": 198.4,
      "ttft_ms": 2580.6
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 1.956,
      "scores": {
        "mmlu": 39.4,
        "hellaswag": 56.9,
        "gsm8k": 21.1,
        "truthfulqa": 41.8,
        "arc_challenge": 35.7
      },
      "composite_score": 39.0,
      "tg_ts": 19.2,
      "pp_ts": 110.2,
      "ttft_ms": 4646.1
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 0.978,
      "scores": {
        "mmlu": 39.2,
        "hellaswag": 56.5,
        "gsm8k": 21.4,
        "truthfulqa": 42.7,
        "arc_challenge": 36.6
      },
      "composite_score": 39.3,
      "tg_ts": 37.0,
      "pp_ts": 228.6,
      "ttft_ms": 2239.7
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 0.807,
      "scores": {
        "mmlu": 39.0,
        "hellaswag": 56.2,
        "gsm8k": 21.0,
        "truthfulqa": 41.8,
        "arc_challenge": 37.4
      },
      "composite_score": 39.1,
      "tg_ts": 47.2,
      "pp_ts": 293.6,
      "ttft_ms": 1743.9
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 0.697,
      "scores": {
        "mmlu": 39.4,
        "hellaswag": 57.1,
        "gsm8k": 20.5,
        "truthfulqa": 41.9,
        "arc_challenge": 36.6
      },
      "composite_score": 39.1,
      "tg_ts": 48.6,
      "pp_ts": 363.7,
      "ttft_ms": 1407.8
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 0.587,
      "scores": {
        "mmlu": 39.2,
        "hellaswag": 55.7,
        "gsm8k": 21.3,
        "truthfulqa": 41.5,
        "arc_challenge": 36.4
      },
      "composite_score": 38.8,
      "tg_ts": 63.4,
      "pp_ts": 430.7,
      "ttft_ms": 1188.8
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 0.489,
      "scores": {
        "mmlu": 39.1,
        "hellaswag": 56.1,
        "gsm8k": 21.4,
        "truthfulqa": 42.3,
        "arc_challenge": 37.3
      },
      "composite_score": 39.2,
      "tg_ts": 77.3,
      "pp_ts": 525.7,
      "ttft_ms": 973.9
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 0.416,
      "scores": {
        "mmlu": 39.2,
        "hellaswag": 55.6,
        "gsm8k": 21.7,
        "truthfulqa": 41.7,
        "arc_challenge": 35.7
      },
      "composite_score": 38.8,
      "tg_ts": 89.8,
      "pp_ts": 605.8,
      "ttft_ms": 845.2
    },
    {
      "model_id": "gemma-3-1b",
      "model_name": "Gemma-3-1B",
      "family": "Gemma",
      "params_b": 1.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.318,
      "scores": {
        "mmlu": 39.7,
        "hellaswag": 56.7,
        "gsm8k": 20.5,
        "truthfulqa": 41.6,
        "arc_challenge": 36.0
      },
      "composite_score": 38.9,
      "tg_ts": 119.0,
      "pp_ts": 791.4,
      "ttft_ms": 647.0
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 7.823,
      "scores": {
        "mmlu": 62.3,
        "hellaswag": 76.2,
        "gsm8k": 52.3,
        "truthfulqa": 51.4,
        "arc_challenge": 54.2
      },
      "composite_score": 59.3,
      "tg_ts": 4.1,
      "pp_ts": 28.2,
      "ttft_ms": 18156.0
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 3.912,
      "scores": {
        "mmlu": 61.2,
        "hellaswag": 74.4,
        "gsm8k": 53.0,
        "truthfulqa": 51.4,
        "arc_challenge": 53.6
      },
      "composite_score": 58.7,
      "tg_ts": 9.3,
      "pp_ts": 58.8,
      "ttft_ms": 8707.5
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 3.227,
      "scores": {
        "mmlu": 62.0,
        "hellaswag": 74.7,
        "gsm8k": 52.2,
        "truthfulqa": 50.8,
        "arc_challenge": 53.9
      },
      "composite_score": 58.7,
      "tg_ts": 11.9,
      "pp_ts": 69.3,
      "ttft_ms": 7388.2
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.787,
      "scores": {
        "mmlu": 61.2,
        "hellaswag": 75.1,
        "gsm8k": 52.2,
        "truthfulqa": 50.8,
        "arc_challenge": 53.1
      },
      "composite_score": 58.5,
      "tg_ts": 12.9,
      "pp_ts": 84.4,
      "ttft_ms": 6066.4
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 2.347,
      "scores": {
        "mmlu": 60.7,
        "hellaswag": 75.1,
        "gsm8k": 51.7,
        "truthfulqa": 51.5,
        "arc_challenge": 54.6
      },
      "composite_score": 58.7,
      "tg_ts": 15.1,
      "pp_ts": 104.4,
      "ttft_ms": 4904.2
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.956,
      "scores": {
        "mmlu": 61.1,
        "hellaswag": 74.5,
        "gsm8k": 50.9,
        "truthfulqa": 50.6,
        "arc_challenge": 53.5
      },
      "composite_score": 58.1,
      "tg_ts": 18.6,
      "pp_ts": 115.9,
      "ttft_ms": 4417.6
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.662,
      "scores": {
        "mmlu": 60.6,
        "hellaswag": 75.4,
        "gsm8k": 51.5,
        "truthfulqa": 50.2,
        "arc_challenge": 53.1
      },
      "composite_score": 58.2,
      "tg_ts": 20.8,
      "pp_ts": 135.4,
      "ttft_ms": 3781.4
    },
    {
      "model_id": "gemma-3-4b",
      "model_name": "Gemma-3-4B",
      "family": "Gemma",
      "params_b": 4.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 1.271,
      "scores": {
        "mmlu": 61.4,
        "hellaswag": 74.8,
        "gsm8k": 51.8,
        "truthfulqa": 51.3,
        "arc_challenge": 53.0
      },
      "composite_score": 58.5,
      "tg_ts": 29.9,
      "pp_ts": 188.9,
      "ttft_ms": 2710.4
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 2.934,
      "scores": {
        "mmlu": 44.1,
        "hellaswag": 58.7,
        "gsm8k": 38.9,
        "truthfulqa": 41.8,
        "arc_challenge": 38.6
      },
      "composite_score": 44.4,
      "tg_ts": 12.4,
      "pp_ts": 79.4,
      "ttft_ms": 6448.4
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 1.467,
      "scores": {
        "mmlu": 44.2,
        "hellaswag": 57.5,
        "gsm8k": 38.2,
        "truthfulqa": 40.5,
        "arc_challenge": 39.1
      },
      "composite_score": 43.9,
      "tg_ts": 25.4,
      "pp_ts": 149.4,
      "ttft_ms": 3427.0
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 1.21,
      "scores": {
        "mmlu": 44.3,
        "hellaswag": 58.0,
        "gsm8k": 37.7,
        "truthfulqa": 41.1,
        "arc_challenge": 39.2
      },
      "composite_score": 44.1,
      "tg_ts": 30.1,
      "pp_ts": 183.3,
      "ttft_ms": 2793.2
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 1.045,
      "scores": {
        "mmlu": 43.0,
        "hellaswag": 57.7,
        "gsm8k": 38.0,
        "truthfulqa": 41.4,
        "arc_challenge": 39.4
      },
      "composite_score": 43.9,
      "tg_ts": 36.0,
      "pp_ts": 205.4,
      "ttft_ms": 2492.7
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 0.88,
      "scores": {
        "mmlu": 43.0,
        "hellaswag": 57.9,
        "gsm8k": 38.1,
        "truthfulqa": 42.1,
        "arc_challenge": 38.4
      },
      "composite_score": 43.9,
      "tg_ts": 41.2,
      "pp_ts": 240.3,
      "ttft_ms": 2130.7
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 0.733,
      "scores": {
        "mmlu": 43.6,
        "hellaswag": 57.5,
        "gsm8k": 36.9,
        "truthfulqa": 41.7,
        "arc_challenge": 39.0
      },
      "composite_score": 43.7,
      "tg_ts": 44.9,
      "pp_ts": 349.3,
      "ttft_ms": 1465.8
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 0.623,
      "scores": {
        "mmlu": 43.3,
        "hellaswag": 57.5,
        "gsm8k": 37.7,
        "truthfulqa": 42.2,
        "arc_challenge": 37.8
      },
      "composite_score": 43.7,
      "tg_ts": 58.4,
      "pp_ts": 413.2,
      "ttft_ms": 1239.1
    },
    {
      "model_id": "deepseek-r1-1.5b",
      "model_name": "DeepSeek-R1-1.5B",
      "family": "DeepSeek",
      "params_b": 1.5,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.477,
      "scores": {
        "mmlu": 43.7,
        "hellaswag": 56.4,
        "gsm8k": 36.9,
        "truthfulqa": 41.8,
        "arc_challenge": 38.2
      },
      "composite_score": 43.4,
      "tg_ts": 77.1,
      "pp_ts": 492.8,
      "ttft_ms": 1039.0
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 3.325,
      "scores": {
        "mmlu": 47.1,
        "hellaswag": 60.9,
        "gsm8k": 29.4,
        "truthfulqa": 43.0,
        "arc_challenge": 40.9
      },
      "composite_score": 44.3,
      "tg_ts": 10.1,
      "pp_ts": 72.5,
      "ttft_ms": 7062.1
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 1.662,
      "scores": {
        "mmlu": 46.4,
        "hellaswag": 61.2,
        "gsm8k": 27.8,
        "truthfulqa": 42.9,
        "arc_challenge": 40.7
      },
      "composite_score": 43.8,
      "tg_ts": 19.4,
      "pp_ts": 135.1,
      "ttft_ms": 3789.8
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 1.371,
      "scores": {
        "mmlu": 46.2,
        "hellaswag": 61.5,
        "gsm8k": 28.0,
        "truthfulqa": 43.1,
        "arc_challenge": 41.0
      },
      "composite_score": 44.0,
      "tg_ts": 26.7,
      "pp_ts": 156.2,
      "ttft_ms": 3277.8
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 1.184,
      "scores": {
        "mmlu": 46.9,
        "hellaswag": 60.5,
        "gsm8k": 28.6,
        "truthfulqa": 42.8,
        "arc_challenge": 41.0
      },
      "composite_score": 44.0,
      "tg_ts": 29.2,
      "pp_ts": 214.2,
      "ttft_ms": 2390.3
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 0.997,
      "scores": {
        "mmlu": 45.4,
        "hellaswag": 60.6,
        "gsm8k": 28.7,
        "truthfulqa": 42.4,
        "arc_challenge": 40.8
      },
      "composite_score": 43.6,
      "tg_ts": 34.2,
      "pp_ts": 247.2,
      "ttft_ms": 2071.2
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 0.831,
      "scores": {
        "mmlu": 46.1,
        "hellaswag": 61.6,
        "gsm8k": 27.1,
        "truthfulqa": 42.3,
        "arc_challenge": 41.2
      },
      "composite_score": 43.7,
      "tg_ts": 42.7,
      "pp_ts": 288.2,
      "ttft_ms": 1776.5
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 0.707,
      "scores": {
        "mmlu": 46.0,
        "hellaswag": 61.2,
        "gsm8k": 28.0,
        "truthfulqa": 43.0,
        "arc_challenge": 40.3
      },
      "composite_score": 43.7,
      "tg_ts": 48.9,
      "pp_ts": 309.9,
      "ttft_ms": 1652.1
    },
    {
      "model_id": "smollm2-1.7b",
      "model_name": "SmolLM2-1.7B",
      "family": "SmolLM",
      "params_b": 1.7,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.54,
      "scores": {
        "mmlu": 46.5,
        "hellaswag": 60.5,
        "gsm8k": 28.1,
        "truthfulqa": 42.8,
        "arc_challenge": 40.6
      },
      "composite_score": 43.7,
      "tg_ts": 63.8,
      "pp_ts": 440.0,
      "ttft_ms": 1163.6
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 5.867,
      "scores": {
        "mmlu": 57.1,
        "hellaswag": 71.9,
        "gsm8k": 41.6,
        "truthfulqa": 48.7,
        "arc_challenge": 50.2
      },
      "composite_score": 53.9,
      "tg_ts": 5.7,
      "pp_ts": 38.6,
      "ttft_ms": 13264.2
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 2.934,
      "scores": {
        "mmlu": 56.0,
        "hellaswag": 71.4,
        "gsm8k": 40.3,
        "truthfulqa": 48.0,
        "arc_challenge": 48.4
      },
      "composite_score": 52.8,
      "tg_ts": 11.4,
      "pp_ts": 73.3,
      "ttft_ms": 6985.0
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 2.42,
      "scores": {
        "mmlu": 56.4,
        "hellaswag": 71.4,
        "gsm8k": 41.1,
        "truthfulqa": 47.9,
        "arc_challenge": 48.7
      },
      "composite_score": 53.1,
      "tg_ts": 15.0,
      "pp_ts": 101.9,
      "ttft_ms": 5024.5
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.09,
      "scores": {
        "mmlu": 54.9,
        "hellaswag": 71.5,
        "gsm8k": 40.4,
        "truthfulqa": 48.1,
        "arc_challenge": 49.2
      },
      "composite_score": 52.8,
      "tg_ts": 15.6,
      "pp_ts": 116.1,
      "ttft_ms": 4410.0
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 1.76,
      "scores": {
        "mmlu": 55.2,
        "hellaswag": 71.0,
        "gsm8k": 40.2,
        "truthfulqa": 47.0,
        "arc_challenge": 49.4
      },
      "composite_score": 52.6,
      "tg_ts": 21.0,
      "pp_ts": 123.1,
      "ttft_ms": 4159.2
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.467,
      "scores": {
        "mmlu": 56.1,
        "hellaswag": 71.6,
        "gsm8k": 39.2,
        "truthfulqa": 47.8,
        "arc_challenge": 49.1
      },
      "composite_score": 52.8,
      "tg_ts": 23.1,
      "pp_ts": 165.7,
      "ttft_ms": 3089.9
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.247,
      "scores": {
        "mmlu": 55.9,
        "hellaswag": 70.9,
        "gsm8k": 40.3,
        "truthfulqa": 47.5,
        "arc_challenge": 48.7
      },
      "composite_score": 52.7,
      "tg_ts": 26.0,
      "pp_ts": 198.5,
      "ttft_ms": 2579.3
    },
    {
      "model_id": "ministral-3b",
      "model_name": "Ministral-3B",
      "family": "Mistral",
      "params_b": 3.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.953,
      "scores": {
        "mmlu": 54.6,
        "hellaswag": 71.4,
        "gsm8k": 39.7,
        "truthfulqa": 48.7,
        "arc_challenge": 49.9
      },
      "composite_score": 52.9,
      "tg_ts": 39.0,
      "pp_ts": 238.6,
      "ttft_ms": 2145.9
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 5.867,
      "scores": {
        "mmlu": 51.7,
        "hellaswag": 68.6,
        "gsm8k": 35.3,
        "truthfulqa": 45.6,
        "arc_challenge": 46.7
      },
      "composite_score": 49.6,
      "tg_ts": 5.5,
      "pp_ts": 43.7,
      "ttft_ms": 11716.2
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 2.934,
      "scores": {
        "mmlu": 50.9,
        "hellaswag": 67.4,
        "gsm8k": 34.7,
        "truthfulqa": 45.2,
        "arc_challenge": 45.3
      },
      "composite_score": 48.7,
      "tg_ts": 12.6,
      "pp_ts": 87.4,
      "ttft_ms": 5858.1
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 2.42,
      "scores": {
        "mmlu": 51.4,
        "hellaswag": 68.8,
        "gsm8k": 34.7,
        "truthfulqa": 45.1,
        "arc_challenge": 45.0
      },
      "composite_score": 49.0,
      "tg_ts": 13.9,
      "pp_ts": 103.8,
      "ttft_ms": 4932.6
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.09,
      "scores": {
        "mmlu": 51.7,
        "hellaswag": 68.0,
        "gsm8k": 35.0,
        "truthfulqa": 45.5,
        "arc_challenge": 45.5
      },
      "composite_score": 49.1,
      "tg_ts": 16.7,
      "pp_ts": 114.8,
      "ttft_ms": 4459.9
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 1.76,
      "scores": {
        "mmlu": 51.2,
        "hellaswag": 67.9,
        "gsm8k": 35.3,
        "truthfulqa": 44.9,
        "arc_challenge": 45.9
      },
      "composite_score": 49.0,
      "tg_ts": 19.4,
      "pp_ts": 143.5,
      "ttft_ms": 3567.9
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.467,
      "scores": {
        "mmlu": 52.1,
        "hellaswag": 67.6,
        "gsm8k": 34.7,
        "truthfulqa": 46.3,
        "arc_challenge": 45.7
      },
      "composite_score": 49.3,
      "tg_ts": 22.0,
      "pp_ts": 167.0,
      "ttft_ms": 3065.9
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.247,
      "scores": {
        "mmlu": 50.3,
        "hellaswag": 67.7,
        "gsm8k": 34.8,
        "truthfulqa": 44.4,
        "arc_challenge": 44.7
      },
      "composite_score": 48.4,
      "tg_ts": 25.9,
      "pp_ts": 177.8,
      "ttft_ms": 2879.6
    },
    {
      "model_id": "qwen2.5-coder-3b",
      "model_name": "Qwen2.5-Coder-3B",
      "family": "Qwen",
      "params_b": 3.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.953,
      "scores": {
        "mmlu": 51.1,
        "hellaswag": 67.4,
        "gsm8k": 34.6,
        "truthfulqa": 44.5,
        "arc_challenge": 45.2
      },
      "composite_score": 48.6,
      "tg_ts": 37.2,
      "pp_ts": 242.5,
      "ttft_ms": 2111.3
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 7.432,
      "scores": {
        "mmlu": 64.5,
        "hellaswag": 73.8,
        "gsm8k": 67.7,
        "truthfulqa": 50.8,
        "arc_challenge": 57.1
      },
      "composite_score": 62.8,
      "tg_ts": 4.4,
      "pp_ts": 29.2,
      "ttft_ms": 17534.2
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 3.716,
      "scores": {
        "mmlu": 64.2,
        "hellaswag": 73.0,
        "gsm8k": 66.9,
        "truthfulqa": 51.0,
        "arc_challenge": 57.0
      },
      "composite_score": 62.4,
      "tg_ts": 9.4,
      "pp_ts": 66.0,
      "ttft_ms": 7757.6
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 3.066,
      "scores": {
        "mmlu": 64.4,
        "hellaswag": 73.6,
        "gsm8k": 66.8,
        "truthfulqa": 51.0,
        "arc_challenge": 56.7
      },
      "composite_score": 62.5,
      "tg_ts": 10.7,
      "pp_ts": 69.1,
      "ttft_ms": 7409.6
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 2.648,
      "scores": {
        "mmlu": 64.0,
        "hellaswag": 73.2,
        "gsm8k": 66.6,
        "truthfulqa": 51.4,
        "arc_challenge": 56.4
      },
      "composite_score": 62.3,
      "tg_ts": 13.8,
      "pp_ts": 82.0,
      "ttft_ms": 6243.9
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 2.23,
      "scores": {
        "mmlu": 63.9,
        "hellaswag": 73.1,
        "gsm8k": 67.0,
        "truthfulqa": 50.2,
        "arc_challenge": 55.9
      },
      "composite_score": 62.0,
      "tg_ts": 14.8,
      "pp_ts": 95.8,
      "ttft_ms": 5344.5
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 1.858,
      "scores": {
        "mmlu": 63.5,
        "hellaswag": 74.0,
        "gsm8k": 66.2,
        "truthfulqa": 50.8,
        "arc_challenge": 55.9
      },
      "composite_score": 62.1,
      "tg_ts": 17.3,
      "pp_ts": 129.6,
      "ttft_ms": 3950.6
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 1.579,
      "scores": {
        "mmlu": 63.6,
        "hellaswag": 73.5,
        "gsm8k": 66.0,
        "truthfulqa": 50.4,
        "arc_challenge": 56.5
      },
      "composite_score": 62.0,
      "tg_ts": 23.5,
      "pp_ts": 151.8,
      "ttft_ms": 3372.9
    },
    {
      "model_id": "phi-4-mini-reasoning",
      "model_name": "Phi-4-Mini-Reasoning",
      "family": "Phi",
      "params_b": 3.8,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 1.208,
      "scores": {
        "mmlu": 63.2,
        "hellaswag": 73.4,
        "gsm8k": 65.9,
        "truthfulqa": 50.0,
        "arc_challenge": 56.1
      },
      "composite_score": 61.7,
      "tg_ts": 28.8,
      "pp_ts": 208.3,
      "ttft_ms": 2458.0
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 13.69,
      "scores": {
        "mmlu": 68.7,
        "hellaswag": 78.5,
        "gsm8k": 71.6,
        "truthfulqa": 54.1,
        "arc_challenge": 59.7
      },
      "composite_score": 66.5,
      "tg_ts": 2.4,
      "pp_ts": 16.2,
      "ttft_ms": 31604.9
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 6.845,
      "scores": {
        "mmlu": 66.5,
        "hellaswag": 77.9,
        "gsm8k": 71.1,
        "truthfulqa": 54.3,
        "arc_challenge": 59.5
      },
      "composite_score": 65.9,
      "tg_ts": 4.6,
      "pp_ts": 33.9,
      "ttft_ms": 15103.2
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 5.647,
      "scores": {
        "mmlu": 67.2,
        "hellaswag": 77.7,
        "gsm8k": 71.2,
        "truthfulqa": 54.9,
        "arc_challenge": 59.1
      },
      "composite_score": 66.0,
      "tg_ts": 6.5,
      "pp_ts": 45.1,
      "ttft_ms": 11352.5
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 4.877,
      "scores": {
        "mmlu": 66.9,
        "hellaswag": 78.0,
        "gsm8k": 70.3,
        "truthfulqa": 53.5,
        "arc_challenge": 58.2
      },
      "composite_score": 65.4,
      "tg_ts": 7.1,
      "pp_ts": 48.5,
      "ttft_ms": 10556.7
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 4.107,
      "scores": {
        "mmlu": 66.6,
        "hellaswag": 76.9,
        "gsm8k": 70.9,
        "truthfulqa": 53.9,
        "arc_challenge": 59.3
      },
      "composite_score": 65.5,
      "tg_ts": 9.0,
      "pp_ts": 57.9,
      "ttft_ms": 8842.8
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 3.423,
      "scores": {
        "mmlu": 67.5,
        "hellaswag": 77.7,
        "gsm8k": 70.9,
        "truthfulqa": 53.7,
        "arc_challenge": 58.3
      },
      "composite_score": 65.6,
      "tg_ts": 11.2,
      "pp_ts": 68.6,
      "ttft_ms": 7463.6
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 2.909,
      "scores": {
        "mmlu": 66.9,
        "hellaswag": 77.7,
        "gsm8k": 71.1,
        "truthfulqa": 54.0,
        "arc_challenge": 58.8
      },
      "composite_score": 65.7,
      "tg_ts": 11.1,
      "pp_ts": 88.1,
      "ttft_ms": 5811.6
    },
    {
      "model_id": "deepseek-r1-7b",
      "model_name": "DeepSeek-R1-7B",
      "family": "DeepSeek",
      "params_b": 7.0,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 2.225,
      "scores": {
        "mmlu": 67.3,
        "hellaswag": 77.9,
        "gsm8k": 69.8,
        "truthfulqa": 55.7,
        "arc_challenge": 58.3
      },
      "composite_score": 65.8,
      "tg_ts": 16.4,
      "pp_ts": 100.6,
      "ttft_ms": 5089.5
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "BF16",
      "bpw": 16.0,
      "file_size_gb": 2.151,
      "scores": {
        "mmlu": 32.9,
        "hellaswag": 50.5,
        "gsm8k": 7.8,
        "truthfulqa": 38.8,
        "arc_challenge": 30.5
      },
      "composite_score": 32.1,
      "tg_ts": 16.6,
      "pp_ts": 112.0,
      "ttft_ms": 4571.4
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q8_0",
      "bpw": 8.0,
      "file_size_gb": 1.076,
      "scores": {
        "mmlu": 31.7,
        "hellaswag": 49.8,
        "gsm8k": 8.4,
        "truthfulqa": 38.2,
        "arc_challenge": 30.3
      },
      "composite_score": 31.7,
      "tg_ts": 33.8,
      "pp_ts": 233.8,
      "ttft_ms": 2189.9
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q6_K",
      "bpw": 6.6,
      "file_size_gb": 0.887,
      "scores": {
        "mmlu": 31.4,
        "hellaswag": 50.6,
        "gsm8k": 8.1,
        "truthfulqa": 39.6,
        "arc_challenge": 30.2
      },
      "composite_score": 32.0,
      "tg_ts": 39.3,
      "pp_ts": 254.7,
      "ttft_ms": 2010.2
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q5_K_M",
      "bpw": 5.7,
      "file_size_gb": 0.766,
      "scores": {
        "mmlu": 31.1,
        "hellaswag": 49.3,
        "gsm8k": 9.6,
        "truthfulqa": 37.8,
        "arc_challenge": 30.2
      },
      "composite_score": 31.6,
      "tg_ts": 50.4,
      "pp_ts": 315.9,
      "ttft_ms": 1620.8
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q4_K_M",
      "bpw": 4.8,
      "file_size_gb": 0.645,
      "scores": {
        "mmlu": 31.1,
        "hellaswag": 49.7,
        "gsm8k": 8.5,
        "truthfulqa": 38.7,
        "arc_challenge": 29.9
      },
      "composite_score": 31.6,
      "tg_ts": 57.4,
      "pp_ts": 342.1,
      "ttft_ms": 1496.6
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q4_0",
      "bpw": 4.0,
      "file_size_gb": 0.538,
      "scores": {
        "mmlu": 30.9,
        "hellaswag": 48.7,
        "gsm8k": 8.0,
        "truthfulqa": 37.5,
        "arc_challenge": 30.1
      },
      "composite_score": 31.0,
      "tg_ts": 60.5,
      "pp_ts": 477.7,
      "ttft_ms": 1071.8
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q3_K_M",
      "bpw": 3.4,
      "file_size_gb": 0.457,
      "scores": {
        "mmlu": 31.2,
        "hellaswag": 49.5,
        "gsm8k": 8.7,
        "truthfulqa": 38.7,
        "arc_challenge": 29.6
      },
      "composite_score": 31.5,
      "tg_ts": 84.6,
      "pp_ts": 555.8,
      "ttft_ms": 921.2
    },
    {
      "model_id": "tinyllama-1.1b",
      "model_name": "TinyLlama-1.1B",
      "family": "TinyLlama",
      "params_b": 1.1,
      "quant": "Q2_K",
      "bpw": 2.6,
      "file_size_gb": 0.35,
      "scores": {
        "mmlu": 31.4,
        "hellaswag": 49.5,
        "gsm8k": 8.4,
        "truthfulqa": 38.5,
        "arc_challenge": 29.2
      },
      "composite_score": 31.4,
      "tg_ts": 105.1,
      "pp_ts": 672.5,
      "ttft_ms": 761.3
    }
  ],
  "pareto_frontier": [
    "DeepSeek-R1-7B BF16"
  ]
};
