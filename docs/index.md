# LocoLLM Documentation

**Frontier AI on a budget. Crazy, right?**

LocoLLM is an open-source experiment in building routed, task-specific fine-tuned language models that run entirely on consumer hardware. One base model, many specialist adapters, zero cloud dependencies.

<div class="grid cards" markdown>

-   **Getting Started**

    ---

    Understand the project's goals, hardware, and philosophy.

    [:octicons-arrow-right-24: Why LocoLLM](why-locollm.md) · [FAQ](faq.md) · [Meet the Lab](meet-the-lab.md)

-   **Guides**

    ---

    Hands-on guides for training adapters, benchmarking, and evaluation.

    [:octicons-arrow-right-24: Adapter Guide](adapter-guide.md) · [Fine-Tuning Primer](finetuning-primer.md)

-   **Reference**

    ---

    Architecture decisions, model selection rationale, and research direction.

    [:octicons-arrow-right-24: Architecture](architecture.md) · [Research Roadmap](research-roadmap.md)

-   **Benchmarks**

    ---

    Interactive results: quality, speed, and efficiency across 14 models x 8 quant levels.

    [:octicons-arrow-right-24: Overview](benchmarks/index.md) · [Quality](benchmarks/quality.md) · [Bang per Bit](benchmarks/bang-per-bit.md)

-   **Decisions (ADRs)**

    ---

    Recorded technical decisions and the reasoning behind them.

    [:octicons-arrow-right-24: ADR Overview](adr/README.md)

</div>

---

**Links:** [Landing Page](https://locollm.org) · [GitHub](https://github.com/michael-borck/loco-llm) · [MIT License](https://github.com/michael-borck/loco-llm/blob/main/LICENSE)
